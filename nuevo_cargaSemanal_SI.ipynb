{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4bbf21",
   "metadata": {},
   "source": [
    "### Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f049ddf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Librerias\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "import numpy as np\n",
    "import snowflake.connector\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from thefuzz import process\n",
    "from dotenv import load_dotenv\n",
    "# Cargar variables desde .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646e71cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "paises_snowflake = {\n",
    "    'brasil': (\"Brasil\", (22,)),\n",
    "    'caricam': (\"CARICAM\", (14, 15, 16, 17, 19, 20)),\n",
    "    'eu': (\"EU\", (8,)),\n",
    "    'paraguay': (\"Paraguay\", (24,)),\n",
    "    'colombia': (\"Colombia\", (6,)),\n",
    "    'ecuador': (\"Ecuador\", (12,)),\n",
    "    'uruguay': (\"Uruguay\", (23,)),\n",
    "    'chile': (\"Chile\", (13,)),\n",
    "    'dominicana': (\"Rep. Dominicana\", (21,)),\n",
    "    'peru': (\"Perú\", (10,))\n",
    "}\n",
    "columnas_sumar = [\n",
    "    'Precio Unitario VTA GLI', 'Unidades Desplazadas',\n",
    "    'Unidades Inv. CEDIS', 'Unidades Inv. Transito', 'Unidades Inv. Tienda',\n",
    "    'Monto Desplazado Bruto ML'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa667060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificar\n",
    "ambiente = \"DEV_\"\n",
    "PaisCarga =\"Colombia\"  # Cambiar por el país deseado\n",
    "\n",
    "# No es necesario modificar\n",
    "PaisCarpeta = paises_snowflake[PaisCarga.lower()][0]\n",
    "PaisSF = paises_snowflake[PaisCarga.lower()][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5acf853",
   "metadata": {},
   "source": [
    "### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "734f41e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_rango_semanas(nombre_archivo):\n",
    "    \"\"\"\n",
    "    Extrae rango de semanas del patrón SO_yyyyww_yyyyww.\n",
    "    Devuelve (inicio, fin) como enteros YYYYWW.\n",
    "    Considera años bisiestos para validación precisa de semana 53.\n",
    "    \"\"\"\n",
    "    # Patrón básico para capturar años y semanas\n",
    "    patron = r'SI_(20\\d{2})(0[1-9]|[1-4]\\d|5[0-3])_(20\\d{2})(0[1-9]|[1-4]\\d|5[0-3])'\n",
    "    match = re.search(patron, nombre_archivo, re.IGNORECASE)\n",
    "    \n",
    "    if not match:\n",
    "        raise ValueError(\"Patrón SI_yyyyww_yyyyww no encontrado o formato inválido.\")\n",
    "\n",
    "    año1, sem1, año2, sem2 = map(int, match.groups())\n",
    "    \n",
    "    # Validar semana 53 solo para años que la tienen\n",
    "    def tiene_semana_53(año):\n",
    "        # Años que tienen 53 semanas: cuando 1 de enero cae en jueves, \n",
    "        # o cuando es año bisiesto y 1 de enero cae en miércoles\n",
    "        from datetime import date\n",
    "        primer_dia = date(año, 1, 1).weekday()  # 0=lunes, 6=domingo\n",
    "        es_bisiesto = año % 4 == 0 and (año % 100 != 0 or año % 400 == 0)\n",
    "        return primer_dia == 3 or (es_bisiesto and primer_dia == 2)\n",
    "    \n",
    "    if sem1 == 53 and not tiene_semana_53(año1):\n",
    "        raise ValueError(f\"El año {año1} no tiene semana 53.\")\n",
    "    if sem2 == 53 and not tiene_semana_53(año2):\n",
    "        raise ValueError(f\"El año {año2} no tiene semana 53.\")\n",
    "    \n",
    "    return año1 * 100 + sem1, año2 * 100 + sem2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc4a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumir_dataframe(df, columnas_sumar,desc,montos_faltan=None):\n",
    " \n",
    "    # Convertir columnas de suma a float si existen y no lo son\n",
    "    columnas_sumar_existentes = [col for col in columnas_sumar if col in df.columns]\n",
    "    for col in columnas_sumar_existentes:\n",
    "        if col in df.columns and not pd.api.types.is_float_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    # Calcular resumen)\n",
    "    suma_totales = df[columnas_sumar_existentes].sum()\n",
    "    if montos_faltan is not None and not montos_faltan.empty:\n",
    "        suma_totales = suma_totales.add(montos_faltan, fill_value=0)\n",
    "\n",
    "    # Unir ambos resultados\n",
    "    resumen = suma_totales.reset_index()\n",
    "    resumen.columns = ['Variable', f'Resumen_{desc}']\n",
    "    # Agregar columna con longitud del DataFrame original\n",
    "    resumen.loc[len(resumen)] = ['Total Filas en df', len(df)]\n",
    "    return resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d122da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportar_rechazos_por_campo(\n",
    "    df: pd.DataFrame,\n",
    "    campo_faltante: str,\n",
    "    columnas_agrupacion: list,\n",
    "    columnas_suma: list,\n",
    "    carpeta_pais_destino: str,\n",
    "    pais: str\n",
    "):\n",
    "    try:\n",
    "        # Filtrar rechazos\n",
    "        df_rechazos = df[df[campo_faltante].isna()]\n",
    "\n",
    "        df_rechazos = df_rechazos.dropna(axis=1, how='all')\n",
    "        if df_rechazos.empty:\n",
    "            print(f\"Sin rechazos de {campo_faltante}\")\n",
    "            return None\n",
    "        # Agrupar datos con las que columnas que sí existan en el DataFrame\n",
    "        columnas_agrupacion = [col for col in columnas_agrupacion if col in df_rechazos.columns]\n",
    "        columnas_suma = [col for col in columnas_suma if col in df_rechazos.columns]\n",
    "        df_rechazos_agrupado = df_rechazos.groupby(columnas_agrupacion, as_index=False)[columnas_suma].sum()\n",
    "        # Eliminar registros donde todas las columnas de suma sean cero\n",
    "        #df_rechazos_agrupado = df_rechazos_agrupado.loc[df_rechazos_agrupado[columnas_suma].sum(axis=1) != 0]\n",
    "        # Agregar columnas adicionales\n",
    "        df_rechazos_agrupado['Fecha Rechazo'] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        #df_rechazos_agrupado['Sem Métrica'] = semana\n",
    "        df_rechazos_agrupado['Métrica'] = 'SO'\n",
    "        df_rechazos_agrupado.rename(columns={'PAISID': 'País', 'Nombre Grupo':'GrpNombre', 'Nombre Cadena/Formato':'Cadena', \n",
    "                                            'GRPID':'GrpID', 'Descripción presentación de producto':'Descripción','Material SAP':'Código (SAP/ERP)', \n",
    "                                            'Cód. Interno ERP':'Código (SAP/ERP)'}, inplace=True)\n",
    "        df_rechazos_agrupado.drop(columns=['SEMID'], inplace=True, errors='ignore')\n",
    "        # Exportar archivo\n",
    "        hoy = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        salida = os.path.join(carpeta_pais_destino, f\"Rechazos_Sellin_{campo_faltante}_{pais}_{hoy}.xlsx\")\n",
    "        df_rechazos_agrupado.to_excel(salida, index=False)\n",
    "        print(f\"Se generaron rechazos de {campo_faltante}: {salida}\")\n",
    "        return df_rechazos_agrupado\n",
    "    except Exception as e:\n",
    "        print(f\"Error al exportar rechazos de {campo_faltante}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d369e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sql_upper(query: str, engine, params=None):\n",
    "    try:\n",
    "        df = pd.read_sql(query, engine, params=params)\n",
    "        df.columns = [col.upper() for col in df.columns]\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error al ejecutar la consulta SQL o procesar los resultados: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f7b84",
   "metadata": {},
   "source": [
    "### Cargar archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58895a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_catalogo_grp = os.getenv(\"RUTA_CATGRUPOS\")\n",
    "df_cat_grupos = pd.read_excel(ruta_catalogo_grp)\n",
    "df_cat_grupos['GrpNombre'] = df_cat_grupos['GrpNombre'].str.strip().str.upper()\n",
    "df_cat_grupos.rename(columns={'GrpID': 'GRPID', 'GrpNombre': 'GRPNOMBRE', 'PaisID':'PAISID'}, inplace=True)\n",
    "df_cat_grupos = df_cat_grupos.replace('-', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a7cc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo más reciente: C:\\\\Users\\\\elsilva\\\\OneDrive - genommalabinternacional\\\\Layout Internacional\\\\data\\Colombia\\Colombia_SO_202527_202534_SI_202522_202534.xlsx\n",
      "[27. 28. 29. 30. 26. 22. 23. 24. 25. nan]\n",
      "Semanas a procesar : 202522, 202534\n"
     ]
    }
   ],
   "source": [
    "ruta_base = os.getenv(\"RUTA_BASE\")\n",
    "ruta_destino = os.getenv(\"RUTA_DESTINO\")\n",
    "\n",
    "carpeta_pais = os.path.join(ruta_base, PaisCarga)\n",
    "carpeta_pais_destino = os.path.join(ruta_destino, PaisCarga)\n",
    "\n",
    "if os.path.exists(carpeta_pais) and os.path.isdir(carpeta_pais):\n",
    "    archivos_xlsx = [\n",
    "        os.path.join(carpeta_pais, f)\n",
    "        for f in os.listdir(carpeta_pais)\n",
    "        if f.endswith(\".xlsx\") and os.path.isfile(os.path.join(carpeta_pais, f))\n",
    "    ]\n",
    "\n",
    "    if not archivos_xlsx:\n",
    "        raise RuntimeError(\"No se encontró ningún archivo .xlsx en la carpeta.\")\n",
    "    # Ordenar por fecha de modificación descendente (más reciente primero)\n",
    "    archivos_xlsx.sort(key=os.path.getmtime, reverse=True)\n",
    "    ruta_archivo = archivos_xlsx[0]\n",
    "    print(f\"Archivo más reciente: {ruta_archivo}\")\n",
    "    try:\n",
    "        df = pd.read_excel(\n",
    "            ruta_archivo,sheet_name=\"LayOut_SellIn\", skiprows=6,engine='openpyxl',\n",
    "            dtype={ #\"Monto Facturación Bruta ML\": float,\"Monto Facturación Neta ML\": float,\"Factor Bruto Neto SellIn\": float, \n",
    "                        'Nombre Cadena/Formato': str,'Nombre Grupo': str,}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error al leer el archivo: {e}\")\n",
    "    print(df['Semana Genomma'].unique())\n",
    "    # Obtener solo las columnas necesarias\n",
    "    df = df.iloc[:, 7:]\n",
    "    df = df.replace('-', '', regex=True)\n",
    "    df = df[\n",
    "        df['EAN'].notna() &\n",
    "        df['Unidades Facturadas'].notna() &\n",
    "        df['Fecha Ult. Día de la semana'].notna()\n",
    "    ]\n",
    "    # Extraer el rango de semanas del nombre del archivo\n",
    "    rango_semanas = extraer_rango_semanas(os.path.basename(ruta_archivo))\n",
    "    semana_inicio, semana_fin = sorted(rango_semanas)\n",
    "    print(F'Semanas a procesar : {semana_inicio}, {semana_fin}')\n",
    "else:\n",
    "    raise RuntimeError(f\"La carpeta del país '{carpeta_pais}' no existe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bcc47a",
   "metadata": {},
   "source": [
    "### Consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e19cbf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder a las variables\n",
    "user = os.getenv(\"USER\")\n",
    "password = os.getenv(\"PASSWORD\")\n",
    "account = os.getenv(\"ACCOUNT\")\n",
    "role = os.getenv(\"ROLE\")\n",
    "# Configura el motor de conexión\n",
    "conn_sf = create_engine(\n",
    "    f'snowflake://{user}:{password}@{account}/?role={role}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f01b7d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar placeholders para cada país\n",
    "placeholders_pais = \", \".join([\"%s\"] * len(PaisSF))\n",
    "#Consulta Snowflake Tiempo\n",
    "Query_catsemanas = f\"\"\"\n",
    "  SELECT *\n",
    "  FROM PRD_STG.GNM_CT.CATSEMANAS\n",
    "  WHERE (SEMANIO * 100 + SEMNUMERO) BETWEEN {semana_inicio} AND {semana_fin};\n",
    "  \"\"\"\n",
    "df_CatSem = read_sql_upper(Query_catsemanas, conn_sf)\n",
    "#Consulta Snowflake TipoComProd\n",
    "Query_Pais =  \"\"\"\n",
    "  SELECT PAIS, PAISID FROM\n",
    "  PRD_CNS_MX.CATALOGOS.VW_DIM_PAIS\n",
    "  \"\"\"\n",
    "df_Pais = read_sql_upper(Query_Pais, conn_sf)\n",
    "df_Pais['PAIS'] = df_Pais['PAIS'].str.strip().str.upper()\n",
    "Query_Productos = f\"\"\"\n",
    "  SELECT PAISID, PROID, PROPSTID, PROPSTCODBARRAS\n",
    "  FROM PRD_CNS_MX.CATALOGOS.VW_ESTRUCTURAPRODUCTOSTOTALPAISES\n",
    "  WHERE PAISID IN ({placeholders_pais})\n",
    "  ORDER BY PROPSTID ASC\n",
    "  \"\"\"\n",
    "df_Prod = read_sql_upper(Query_Productos, conn_sf, PaisSF)\n",
    "df_Prod = df_Prod.drop_duplicates(subset=['PROPSTCODBARRAS','PAISID'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4fd50",
   "metadata": {},
   "source": [
    "### Filtrar archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5125a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_contar = [\n",
    "    'Nombre Grupo', 'Nombre Cadena/Formato', 'EAN',\n",
    "    'Material SAP', 'Descripción presentación de producto'\n",
    "]\n",
    "\n",
    "columnas_sumar = [\n",
    " \"Unidades Facturadas\", \"Monto Facturación Bruta ML\", \"Monto Facturación Neta ML\", \"Factor Bruto Neto SellIn\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39f21343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CatSem['SEMFIN'] = pd.to_datetime(df_CatSem['SEMFIN'], errors='coerce')\n",
    "# Ejemplo de DataFrame con intervalos\n",
    "df_CatSem = df_CatSem.sort_values('SEMINICIO')\n",
    "if df.empty:\n",
    "    raise RuntimeError(\"El DataFrame está vacío después de la carga.\")\n",
    "# DataFrame con fechas a unir\n",
    "df['Fecha Ult. Día de la semana'] = pd.to_datetime(df['Fecha Ult. Día de la semana'], errors='coerce')\n",
    "df = df.sort_values('Fecha Ult. Día de la semana')\n",
    "# merge_asof une hacia atrás: buscamos el último SEMINICIO <= fecha\n",
    "df_merge = pd.merge_asof(\n",
    "    df,\n",
    "    df_CatSem,\n",
    "    left_on='Fecha Ult. Día de la semana',\n",
    "    right_on='SEMINICIO',\n",
    "    direction='backward'\n",
    ")\n",
    "# Luego validamos que fecha <= SEMFIN\n",
    "df_filtrado = df_merge[df_merge['Fecha Ult. Día de la semana'] <= df_merge['SEMFIN']]\n",
    "if df_filtrado.empty:\n",
    "    raise RuntimeError(\"El DataFrame está vacío después de la carga.\")\n",
    "df_filtrado = df_filtrado.copy()\n",
    "df_filtrado['Pais Nombre']= df_filtrado['Pais Nombre'].str.upper().str.strip()\n",
    "df_filtrado['Nombre Grupo'] = df_filtrado['Nombre Grupo'].str.upper().str.strip()\n",
    "# df_filtrado['Nombre Cadena/Formato'] = df_filtrado['Nombre Cadena/Formato'].str.upper().str.strip()\n",
    "# Alternativa usando expresiones regulares para eliminar decimales .0 si existen y quitar espacios\n",
    "df_filtrado['EAN'] = df_filtrado['EAN'].astype(str).str.replace(r'\\.0$', '', regex=True).str.strip()\n",
    "# Ver un resumen iinicial\n",
    "resumen = resumir_dataframe(df_filtrado, columnas_sumar, 'df_inicial')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4dc793",
   "metadata": {},
   "source": [
    "### Rechazos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea9609d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se generaron rechazos de GRPID: C:\\\\Users\\\\elsilva\\\\OneDrive - genommalabinternacional\\\\Layout Internacional\\\\data\\\\Rechazos\\Colombia\\Rechazos_Sellin_GRPID_Colombia_2025-09-01.xlsx\n",
      "Se generaron rechazos de PROPSTID: C:\\\\Users\\\\elsilva\\\\OneDrive - genommalabinternacional\\\\Layout Internacional\\\\data\\\\Rechazos\\Colombia\\Rechazos_Sellin_PROPSTID_Colombia_2025-09-01.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Unir los dataframes por las claves correspondientes\n",
    "df_merged = df_filtrado.merge(df_Pais[['PAIS', 'PAISID']],\n",
    "                left_on='Pais Nombre',\n",
    "                right_on='PAIS',\n",
    "                how='left')\n",
    "# Hacer el merge para obtener grpid\n",
    "df_merged2 = df_merged.merge(df_cat_grupos[['PAISID','GRPNOMBRE', 'GRPID']].drop_duplicates(),\n",
    "                            left_on =['PAISID','Nombre Grupo'],\n",
    "                            right_on = ['PAISID','GRPNOMBRE'],\n",
    "                            how='left')\n",
    "\n",
    "rechazo1 = exportar_rechazos_por_campo(df_merged2,'GRPID', ['PAISID',\"Nombre Grupo\",\"Nombre Cadena/Formato\",\"SEMID\", 'SEMNUMERO'], [\"Unidades Desplazadas\",\"Monto Desplazado Bruto ML\"], \n",
    "                            carpeta_pais_destino, PaisCarga)\n",
    "\n",
    "# Convetir al mismo tipo de dato\n",
    "df_Prod['PROPSTCODBARRAS'] = df_Prod['PROPSTCODBARRAS'].astype(str).str.replace(r'\\.0$', '', regex=True).str.strip()\n",
    "# Unir los dataframes por Productos\n",
    "df_merged3 = df_merged2.merge(df_Prod,\n",
    "                left_on=['PAISID','EAN'],\n",
    "                right_on=['PAISID','PROPSTCODBARRAS'],\n",
    "                how='left')\n",
    "rechazo2 = exportar_rechazos_por_campo(df_merged3,'PROPSTID', ['PAISID',\"EAN\",\"Descripción presentación de producto\",\"SEMID\",\"Cód. Interno ERP\",\"Material SAP\", 'SEMNUMERO'], \n",
    "                                        [\"Unidades Desplazadas\",\"Monto Desplazado Bruto ML\"], \n",
    "                                        carpeta_pais_destino, PaisCarga)\n",
    "# Resumen del último merge\n",
    "resumen2 = resumir_dataframe(df_merged3, columnas_sumar,'df_merged')\n",
    "# Obtener registros que faltan\n",
    "registros_faltan = df_merged3[df_merged3[[\"GRPID\", \"PROPSTID\"]].isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c96048",
   "metadata": {},
   "source": [
    "### Agrupación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7543328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montos faltantes\n",
    "if not registros_faltan.empty:\n",
    "    # Resumen del último merge\n",
    "    resumen_faltan = resumir_dataframe(registros_faltan, columnas_sumar,'df_faltan')\n",
    "    montos_faltan = registros_faltan[columnas_sumar].sum()\n",
    "else:\n",
    "    montos_faltan = pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bce45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_merged3.empty:\n",
    "    raise Exception(\"El DataFrame está vacío. Revisa los cruces anteriores.\")\n",
    "# Filtrar filas válidas (sin valores nulos en campos clave)\n",
    "df_validado = df_merged3.dropna(subset=['GRPID', 'PROPSTID'])\n",
    "resumen3 = resumir_dataframe(df_validado, columnas_sumar,'df_sin_nulos', montos_faltan)\n",
    "# Renombrar solo las columnas que realmente cambian\n",
    "df_validado = df_validado.copy()\n",
    "df_validado.rename(columns={\n",
    "    \"PAIS\": \"PAISNOM\",\n",
    "    \"SEMANIO\": \"ANOGLI\",\n",
    "    \"SEMNUMERO\": \"SEMGLI\",\n",
    "    \"SEMMES\": \"MESGLI\",\n",
    "    \"Fecha Ult. Día de la semana\": \"ULTIMODIASEM\",\n",
    "    \"Nombre Grupo\": \"NOMGRUPO\",\n",
    "    \"Material SAP\": \"CODERP\",\n",
    "    #\"Cód. Interno ERP\": \"CODERP\",  # Si quieres conservar ambos, usa otro nombre para uno\n",
    "    \"Cód. Cliente SellOut\": \"CODCTESELLOUT\",\n",
    "    \"Descripción presentación de producto\": \"PROPSTDESCERP\",\n",
    "    \"Precio Factura ML\": \"PRECIOFACTML\",\n",
    "    \"Unidades Facturadas\": \"UNFACT\",\n",
    "    \"Monto Facturación Bruta ML\": \"MONTOFACTBRTML\",\n",
    "    \"Monto Facturación Neta ML\": \"MONTOFACTNETAML\",\n",
    "    \"Factor Bruto Neto SellIn\": \"FACTBRUTONETOSELLIN\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Identificar tipos de columnas automáticamente\n",
    "numeric_cols = df_validado.select_dtypes(include=['number']).columns\n",
    "text_cols = df_validado.select_dtypes(include=['object', 'string']).columns\n",
    "\n",
    "# Rellenar\n",
    "df_validado[numeric_cols] = df_validado[numeric_cols].fillna(0)\n",
    "df_validado[text_cols] = df_validado[text_cols].fillna('')\n",
    "# Asegurarse de que las columnas clave estén en el DataFrame\n",
    "columnas_agrupan = [\n",
    "    'SEMID', 'ANOGLI', 'MESGLI', 'SEMGLI', 'PAISID', 'GRPID', 'PROPSTID',\n",
    "    'PAISNOM', 'ULTIMODIASEM', 'NOMGRUPO', 'EAN', 'CODERP', 'CODCTESELLOUT',\n",
    "    'PROPSTDESCERP'\n",
    "    # ← NO incluir las columnas numéricas que vas a agregar\n",
    "]\n",
    "# Agrupar por claves y sumar los valores numéricos\n",
    "for col in [\"PRECIOFACTML\", \"UNFACT\", \"MONTOFACTBRTML\", \"MONTOFACTNETAML\", \"FACTBRUTONETOSELLIN\"]:\n",
    "    if col in df_validado.columns:\n",
    "        df_validado[col] = df_validado[col].fillna(0)\n",
    "df_validado = df_validado.dropna(axis=1, how='all')\n",
    "# Filtrar columnas que sí existen en el DataFrame\n",
    "columnas_agrupan = [col for col in columnas_agrupan if col in df_validado.columns]\n",
    "df_grouped = df_validado.groupby(\n",
    "    columnas_agrupan,\n",
    "    as_index=False\n",
    ").agg({\n",
    "    \"PRECIOFACTML\": 'mean',\n",
    "    \"UNFACT\": \"sum\",\n",
    "    \"MONTOFACTBRTML\": \"sum\",\n",
    "    \"MONTOFACTNETAML\": \"sum\",\n",
    "    \"FACTBRUTONETOSELLIN\": \"sum\"\n",
    "})\n",
    "if df_grouped.empty:\n",
    "    raise Exception(\"El DataFrame está vacío.\")\n",
    "# Añadir fechas\n",
    "df_grouped[['CREATED_AT', 'UPDATED_AT']] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "columnas_objetivo = [\n",
    "'SEMID', 'ANOGLI', 'MESGLI', 'SEMGLI', 'PAISID', 'GRPID', 'PROPSTID',\n",
    "'PAISNOM', 'ULTIMODIASEM', 'NOMGRUPO', 'EAN', 'CODERP', 'CODCTESELLOUT',\n",
    "'PROPSTDESCERP', 'PRECIOFACTML', 'UNFACT', 'MONTOFACTBRTML',\n",
    "'MONTOFACTNETAML', 'FACTBRUTONETOSELLIN', 'CREATED_AT', 'UPDATED_AT'\n",
    "]\n",
    "# Reindexa columnas: agrega las que falten con NaN, y ordena según columnas_objetivo\n",
    "df_final = df_grouped.reindex(columns=columnas_objetivo)\n",
    "\n",
    "if not montos_faltan.empty:\n",
    "    montos_faltan = montos_faltan.rename({\n",
    "        \"Unidades Facturadas\": \"UNFACT\",\n",
    "        \"Monto Facturación Bruta ML\": \"MONTOFACTBRTML\",\n",
    "        \"Monto Facturación Neta ML\": \"MONTOFACTNETAML\",\n",
    "        \"Factor Bruto Neto SellIn\": \"FACTBRUTONETOSELLIN\"\n",
    "    })\n",
    "# Resúmenes finales\n",
    "resumen4 = resumir_dataframe(df_grouped, [ 'UNFACT', 'MONTOFACTBRTML',\n",
    "       'MONTOFACTNETAML', 'FACTBRUTONETOSELLIN'],'df_calculado',montos_faltan)\n",
    "resumen5 = resumir_dataframe(df_grouped, [ 'UNFACT', 'MONTOFACTBRTML',\n",
    "       'MONTOFACTNETAML', 'FACTBRUTONETOSELLIN'], 'df_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573f21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Resumen_df_inicial</th>\n",
       "      <th>Resumen_df_merged</th>\n",
       "      <th>Resumen_df_sin_nulos</th>\n",
       "      <th>Resumen_df_calculado</th>\n",
       "      <th>Resumen_df_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FACTBRUTONETOSELLIN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MONTOFACTBRTML</td>\n",
       "      <td>63,014,771,777.37</td>\n",
       "      <td>63,014,771,777.37</td>\n",
       "      <td>63,014,771,777.37</td>\n",
       "      <td>63,014,771,777.37</td>\n",
       "      <td>59,008,072,660.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MONTOFACTNETAML</td>\n",
       "      <td>48,937,105,022.93</td>\n",
       "      <td>48,937,105,022.93</td>\n",
       "      <td>48,937,105,022.93</td>\n",
       "      <td>48,937,105,022.93</td>\n",
       "      <td>45,221,351,933.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Total Filas en df</td>\n",
       "      <td>43,891.00</td>\n",
       "      <td>43,891.00</td>\n",
       "      <td>40,488.00</td>\n",
       "      <td>20,274.00</td>\n",
       "      <td>20,274.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNFACT</td>\n",
       "      <td>2,446,702.00</td>\n",
       "      <td>2,446,702.00</td>\n",
       "      <td>2,446,702.00</td>\n",
       "      <td>2,446,702.00</td>\n",
       "      <td>2,287,511.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Variable  Resumen_df_inicial  Resumen_df_merged  \\\n",
       "0  FACTBRUTONETOSELLIN                0.00               0.00   \n",
       "1       MONTOFACTBRTML   63,014,771,777.37  63,014,771,777.37   \n",
       "2      MONTOFACTNETAML   48,937,105,022.93  48,937,105,022.93   \n",
       "3    Total Filas en df           43,891.00          43,891.00   \n",
       "4               UNFACT        2,446,702.00       2,446,702.00   \n",
       "\n",
       "   Resumen_df_sin_nulos  Resumen_df_calculado  Resumen_df_final  \n",
       "0                  0.00                  0.00              0.00  \n",
       "1     63,014,771,777.37     63,014,771,777.37 59,008,072,660.37  \n",
       "2     48,937,105,022.93     48,937,105,022.93 45,221,351,933.69  \n",
       "3             40,488.00             20,274.00         20,274.00  \n",
       "4          2,446,702.00          2,446,702.00      2,287,511.00  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vista de todos los resúmenes\n",
    "resumen_final = resumen.merge(resumen2, on=\"Variable\", how=\"outer\")\n",
    "resumen_final = resumen_final.merge(resumen3, on=\"Variable\", how=\"outer\")\n",
    "resumen_final.replace({ \"Unidades Facturadas\": \"UNFACT\",\n",
    "                  \"Factor Bruto Neto SellIn\": \"FACTBRUTONETOSELLIN\",\n",
    "                  \"Monto Facturación Bruta ML\": \"MONTOFACTBRTML\",\n",
    "                   \"Monto Facturación Neta ML\": \"MONTOFACTNETAML\"}, inplace=True)\n",
    "resumen_final = resumen_final.merge(resumen4, on=\"Variable\", how=\"outer\")\n",
    "resumen_final = resumen_final.merge(resumen5, on=\"Variable\", how=\"outer\")\n",
    "resumen_final.replace(np.nan, '', inplace=True)\n",
    "resumen_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7a6bd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se generó el archivo final: \\\\NASPRO.infovisiontv.com\\DGI\\DGIBancoCentral\\27SIInternacional\\CARGA_Colombia_SEM_202522_202534.xlsx\n"
     ]
    }
   ],
   "source": [
    "ruta_destino = os.path.join(r'\\\\NASPRO.infovisiontv.com\\DGI\\DGIBancoCentral\\27SIInternacional', f\"CARGA_{PaisCarga}_SEM_{semana_inicio}_{semana_fin }.xlsx\")\n",
    "try:\n",
    "    df_final.to_excel(ruta_destino, index=False)\n",
    "    print(f\"Se generó el archivo final: {ruta_destino}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error al guardar el archivo final: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
